# hydra:
#   run:
#     dir: ./my_output_dir  # save outputs to custom dir

root_dir: ${hydra:runtime.cwd}  # 运行代码的根目录


dataset_name: owt     # `TinyStories` or `owt`, change as needed
dataset_split: train          # `train` or `valid`, change as needed

data_dir: ${root_dir}/data
txt_dir: ${data_dir}
dat_dir: ${data_dir}/dat
train_txt_path: ${txt_dir}/TinyStoriesV2-GPT4-train.txt
valid_txt_path: ${txt_dir}/TinyStoriesV2-GPT4-valid.txt

input_path: ${data_dir}/TinyStoriesV2-GPT4-train.txt    # txt data for training tokenizer 用这个训练一个 tokenizer
train_dat_path: ${dat_dir}/TinyStoriesV2-GPT4-train.dat # 是 token ID 序列，而非 token 字符串和 ID 的映射关系
valid_dat_path: ${dat_dir}/TinyStoriesV2-GPT4-valid.dat

tokenizer_dir: ${root_dir}/tokenizer_data # save path for tokenizer
vocab_path: ${tokenizer_dir}/vocab.pkl
merges_path: ${tokenizer_dir}/merges.pkl


vocab_size_map:
  TinyStories: 10_000
  owt: 32_000

# 根据 dataset_name 动态设置 vocab_size
vocab_size: ${vocab_size_map.${dataset_name}}
special_tokens: ["<|endoftext|>"]

# train tokenizer config
num_chunks: 32
num_processes: 8


# encode config
batch_size: 10000 # 4096
n_workers: 8 # 8